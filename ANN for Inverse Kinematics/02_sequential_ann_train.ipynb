{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size and model save paths and epochs\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PATH = 'Models/Final/02_Sequential_10M/'\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "data = pd.read_csv('ik_dataset_4.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the data \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data))\n",
    "data.columns = ['joint_0','joint_1','joint_2','joint_3','joint_4','joint_5','x','y','z']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "\n",
    "data_train, data_test = train_test_split(data, train_size=0.8, random_state=42)\n",
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the data to tensors\n",
    "# We convert each column into seperate tensors and vertically stack them at the end so that more space is not used for each of the model\n",
    "\n",
    "# Train\n",
    "x_train = torch.Tensor(data_train['x'].to_numpy()).to(device)\n",
    "y_train = torch.Tensor(data_train['y'].to_numpy()).to(device)\n",
    "z_train = torch.Tensor(data_train['z'].to_numpy()).to(device)\n",
    "joint_0_train = torch.Tensor(data_train['joint_0'].to_numpy()).to(device)\n",
    "joint_1_train = torch.Tensor(data_train['joint_1'].to_numpy()).to(device)\n",
    "joint_2_train = torch.Tensor(data_train['joint_2'].to_numpy()).to(device)\n",
    "joint_3_train = torch.Tensor(data_train['joint_3'].to_numpy()).to(device)\n",
    "joint_4_train = torch.Tensor(data_train['joint_4'].to_numpy()).to(device)\n",
    "joint_5_train = torch.Tensor(data_train['joint_5'].to_numpy()).to(device)\n",
    "\n",
    "# Test\n",
    "x_test = torch.Tensor(data_test['x'].to_numpy()).to(device)\n",
    "y_test = torch.Tensor(data_test['y'].to_numpy()).to(device)\n",
    "z_test = torch.Tensor(data_test['z'].to_numpy()).to(device)\n",
    "joint_0_test = torch.Tensor(data_test['joint_0'].to_numpy()).to(device)\n",
    "joint_1_test = torch.Tensor(data_test['joint_1'].to_numpy()).to(device)\n",
    "joint_2_test = torch.Tensor(data_test['joint_2'].to_numpy()).to(device)\n",
    "joint_3_test = torch.Tensor(data_test['joint_3'].to_numpy()).to(device)\n",
    "joint_4_test = torch.Tensor(data_test['joint_4'].to_numpy()).to(device)\n",
    "joint_5_test = torch.Tensor(data_test['joint_5'].to_numpy()).to(device)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the model \n",
    "\n",
    "class InverseKinematicsABBIRB140(nn.Module):\n",
    "    \n",
    "    \"\"\" This is a model structure specifically designed to calculate the inverse kinematics of the industrial robot ABB IRB 140, but can be used for any robot\n",
    "        This has a layer structure of input -> 64 -> 128 -> 64 -> output with non-linear activation and normalisation layers\n",
    "    Args:\n",
    "        input_features (int): defines the number of features given as input to the model\n",
    "        output_features (int): defines the number of features the model outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.layer_stack_1 = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer_stack_2 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer_stack_3 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer_stack_4 = nn.Sequential(\n",
    "            nn.Linear(in_features=128, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=128, out_features=output_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_stack_1(x)\n",
    "        x = self.layer_stack_2(x)\n",
    "        x = self.layer_stack_3(x)\n",
    "        x = self.layer_stack_4(x)\n",
    "        return x   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training step\n",
    "def train_step(model: torch.nn.Module,\n",
    "               batch: tuple,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    xtrain, ytrain = batch\n",
    "    xtrain, ytrain = xtrain.to(device), ytrain.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    ypreds = model(xtrain)\n",
    "    loss = loss_fn(ypreds, ytrain)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the training time\n",
    "\n",
    "def print_train_time(start:float, end:float, device:torch.device=None):\n",
    "    total_time = end-start\n",
    "    print(f\"Train time on device {device} : {total_time:.4f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing step\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              batch: tuple,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    \n",
    "    xtest, ytest = batch\n",
    "    xtest, ytest = xtest.to(device), ytest.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        ypreds = model(xtest)\n",
    "        loss = loss_fn(ypreds, ytest)\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 0\n",
    "\n",
    "NAME_0 = 'model_0.pth'\n",
    "MODEL_SAVE_PATH_0 = PATH + NAME_0\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train), dim=1), joint_0_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test), dim=1), joint_0_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_0 = InverseKinematicsABBIRB140(input_features=3, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 0\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_0 = timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_0, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_0, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_0 = timer()\n",
    "train_time_model_0 = print_train_time(start=train_time_start_model_0, end=train_time_end_model_0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_0 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH_0)\n",
    "model_0_dict = {'Model name' : 'Model 0', 'Training time' : train_time_model_0, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 1\n",
    "\n",
    "NAME_1 = 'model_1.pth'\n",
    "MODEL_SAVE_PATH_1 = PATH + NAME_1\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train, joint_0_train), dim=1), joint_1_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test, joint_0_test), dim=1), joint_1_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_1 = InverseKinematicsABBIRB140(input_features=4, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_1 = timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_1, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_1, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_1 = timer()\n",
    "train_time_model_1 = print_train_time(start=train_time_start_model_1, end=train_time_end_model_1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_1 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_1.state_dict(), f=MODEL_SAVE_PATH_1)\n",
    "model_1_dict = {'Model name' : 'Model 1', 'Training time' : train_time_model_1, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 2\n",
    "\n",
    "NAME_2 = 'model_2.pth'\n",
    "MODEL_SAVE_PATH_2 = PATH + NAME_2\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train, joint_0_train, joint_1_train), dim=1), joint_2_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test, joint_0_test, joint_1_test), dim=1), joint_2_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_2 = InverseKinematicsABBIRB140(input_features=5, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_2 = timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_2, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_2, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_2 = timer()\n",
    "train_time_model_2 = print_train_time(start=train_time_start_model_2, end=train_time_end_model_2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_2 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_2.state_dict(), f=MODEL_SAVE_PATH_2)\n",
    "model_2_dict = {'Model name' : 'Model 2', 'Training time' : train_time_model_2, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 3\n",
    "\n",
    "NAME_3 = 'model_3.pth'\n",
    "MODEL_SAVE_PATH_3 = PATH + NAME_3\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train, joint_0_train, joint_1_train, joint_2_train), dim=1), joint_3_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test, joint_0_test, joint_1_test, joint_2_test), dim=1), joint_3_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_3 = InverseKinematicsABBIRB140(input_features=6, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 3\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_3 = timer()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_3, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_3, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_3 = timer()\n",
    "train_time_model_3 = print_train_time(start=train_time_start_model_3, end=train_time_end_model_3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_3 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_3.state_dict(), f=MODEL_SAVE_PATH_3)\n",
    "model_3_dict = {'Model name' : 'Model 3', 'Training time' : train_time_model_3, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 4\n",
    "\n",
    "NAME_4 = 'model_4.pth'\n",
    "MODEL_SAVE_PATH_4 = PATH + NAME_4\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train, joint_0_train, joint_1_train, joint_2_train, joint_3_train), dim=1), joint_4_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test, joint_0_test, joint_1_test, joint_2_test, joint_3_test), dim=1), joint_4_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_4 = InverseKinematicsABBIRB140(input_features=7, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_4.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_4 = timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_4, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_4, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_4 = timer()\n",
    "train_time_model_4 = print_train_time(start=train_time_start_model_4, end=train_time_end_model_4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_4 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_4.state_dict(), f=MODEL_SAVE_PATH_4)\n",
    "model_4_dict = {'Model name' : 'Model 4', 'Training time' : train_time_model_4, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model 5\n",
    "\n",
    "NAME_5 = 'model_5.pth'\n",
    "MODEL_SAVE_PATH_5 = PATH + NAME_5\n",
    "\n",
    "train_dataset = TensorDataset(torch.stack((x_train, y_train ,z_train, joint_0_train, joint_1_train, joint_2_train, joint_3_train, joint_4_train), dim=1), joint_5_train.unsqueeze(dim=1))\n",
    "test_dataset = TensorDataset(torch.stack((x_test, y_test ,z_test, joint_0_test, joint_1_test, joint_2_test, joint_3_test, joint_4_test), dim=1), joint_5_test.unsqueeze(dim=1))\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle = False)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_5 = InverseKinematicsABBIRB140(input_features=8, output_features=1).to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=model_5.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model 5\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_model_5 = timer()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "\n",
    "        train_loss = 0\n",
    "        # Training loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = train_step(model=model_5, batch=batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "            train_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Training)')\n",
    "            t.set_postfix({\n",
    "                'Train Batch loss': batch_loss,\n",
    "                'Train loss': train_loss/(i+1)\n",
    "            })\n",
    "                \n",
    "    with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "        test_loss = 0\n",
    "        # Testing loop\n",
    "        for i, batch in t:\n",
    "            batch_loss = test_step(model=model_5, batch=batch, loss_fn=loss_fn)\n",
    "            test_loss += batch_loss\n",
    "            t.set_description(f'Epoch [{epoch+1}/{epochs}] (Testing) ')\n",
    "            t.set_postfix({\n",
    "                'Test batch loss': batch_loss,\n",
    "                'Test loss': test_loss /(i+1)\n",
    "            })\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "train_time_end_model_5 = timer()\n",
    "train_time_model_5 = print_train_time(start=train_time_start_model_5, end=train_time_end_model_5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model_5 and its training time, loss values\n",
    "\n",
    "torch.save(obj=model_5.state_dict(), f=MODEL_SAVE_PATH_5)\n",
    "model_5_dict = {'Model name' : 'Model 5', 'Training time' : train_time_model_5, 'Train loss' : train_loss/len(train_dataloader), 'Test loss' : test_loss/len(test_dataloader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the training time and loss of various models \n",
    "\n",
    "compare_models = pd.DataFrame([model_0_dict, model_1_dict, model_2_dict, model_3_dict, model_4_dict, model_5_dict])\n",
    "compare_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
